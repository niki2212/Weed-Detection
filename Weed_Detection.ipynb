{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee2a424",
   "metadata": {},
   "source": [
    "# Image processing to detect crops and weeds in images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4998c0c",
   "metadata": {},
   "source": [
    "### Approach 1: Using only image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a82a97",
   "metadata": {},
   "source": [
    "#### Import the neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792601c",
   "metadata": {},
   "source": [
    "### For a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda2428",
   "metadata": {},
   "source": [
    "#### 1. Load the image\n",
    "#### 2. Split it into red, green and blue components\n",
    "#### 3. Subtract the blue and green component and just get the green component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b077f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_origin = cv2.imread(r'C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\weed.png')\n",
    "p = cv2.resize(p_origin.copy(), (400,400))\n",
    "\n",
    "#Split the red, green, and blue components\n",
    "p_blue, p_green, p_red = cv2.split(p)\n",
    "\n",
    "#Getting the mask by subtracting red and blue componenets\n",
    "p_excess_green = 128 + np.int16(p_green) - np.int16(p_blue) + np.int16(p_green) - np.int16(p_red)\n",
    "p_excess_green = np.uint8(np.clip(p_excess_green, 0, 255))\n",
    "\n",
    "mask = np.uint8((p_excess_green > 200) * 1)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(p, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Excess green image')\n",
    "plt.imshow(p_excess_green, cmap='gray')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Mask Image')\n",
    "plt.imshow(mask, cmap='gray')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('(128 + G-R + G-B) Histogram')\n",
    "plt.hist(p_excess_green.ravel(), bins=256, range=[0, 256])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9687feb",
   "metadata": {},
   "source": [
    "#### Draw contours around the masks obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993423b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To draw contours\n",
    "p_contoured = p.copy()\n",
    "\n",
    "contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "cv2.drawContours(p_contoured, contours, -1, (255, 0, 0), 3)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(p_contoured, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17548c",
   "metadata": {},
   "source": [
    "#### Draw boxes around the contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw box contours \n",
    "boxes = []\n",
    "\n",
    "for c in contours:\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    if(w > 20 and h > 20):\n",
    "        boxes.append((x, y, w, h))\n",
    "        \n",
    "p_temp = p.copy()\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(p_temp, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (0, 0, 255), 4)\n",
    "    \n",
    "plt.imshow(p_temp)\n",
    "plt.show()\n",
    "\n",
    "print(len(boxes))\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1c2d5",
   "metadata": {},
   "source": [
    "### Approach 2: Using image processing and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b72d0",
   "metadata": {},
   "source": [
    "#### 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import io, exposure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99ad85",
   "metadata": {},
   "source": [
    "#### 2. Draw bounding boxes around the YOLO co-ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02312893",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Draw bounding boxes based on the coordinates\n",
    "directory = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\data\"\n",
    "output_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Bounding_boxes\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#Loading images\n",
    "image_files = [file for file in os.listdir(directory) if file.endswith(\".jpg\") or file.endswith(\".jpeg\")]\n",
    "\n",
    "#Define class colors\n",
    "class_colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255)]  # Example: Green, Red, Blue\n",
    "\n",
    "#Process each image\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(directory, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "\n",
    "    annotation_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    annotation_path = os.path.join(directory, annotation_file)\n",
    "\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        annotations = file.readlines()\n",
    "\n",
    "    #Process each annotation\n",
    "    for annotation in annotations:\n",
    "        class_id, x, y, w, h = map(float, annotation.split())\n",
    "\n",
    "        # Convert the normalized coordinates to pixel values\n",
    "        x_min = int((x - w/2) * width)\n",
    "        y_min = int((y - h/2) * height)\n",
    "        x_max = int((x + w/2) * width)\n",
    "        y_max = int((y + h/2) * height)\n",
    "\n",
    "        # Get the class color based on the class_id\n",
    "        color = class_colors[int(class_id)]\n",
    "\n",
    "        #Drawing bounding boxes\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "\n",
    "    #Save the image with bounding boxes\n",
    "    output_path = os.path.join(output_folder, image_file)\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "#     print(f\"Processed: {image_file}\")\n",
    "\n",
    "print(\"Bounding boxes drawn and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598507c5",
   "metadata": {},
   "source": [
    "#### 3. Crop the images from the bounding boxes and put them in appropriate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop the images from the bounding boxes\n",
    "\n",
    "directory = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\data\"\n",
    "output_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "image_files = [file for file in os.listdir(directory) if file.endswith(\".jpg\") or file.endswith(\".jpeg\")]\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "\n",
    "    image_path = os.path.join(directory, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    annotation_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    annotation_path = os.path.join(directory, annotation_file)\n",
    "\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        annotations = file.readlines()\n",
    "\n",
    "    # Process each annotation\n",
    "    for i, annotation in enumerate(annotations):\n",
    "        class_id, x, y, w, h = map(float, annotation.split())\n",
    "\n",
    "        # Convert the normalized coordinates to pixel values\n",
    "        x_min = int((x - w/2) * width)\n",
    "        y_min = int((y - h/2) * height)\n",
    "        x_max = int((x + w/2) * width)\n",
    "        y_max = int((y + h/2) * height)\n",
    "\n",
    "        # Crop the image based on the bounding box\n",
    "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        class_folder = os.path.join(output_folder, f\"class_{int(class_id)}\")\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(class_folder, f\"{image_file}_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_path, cropped_image, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "\n",
    "    #print(f\"Processed: {image_file}\")\n",
    "print(\"Images extracted from bounding boxes and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947229ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put these images in two folders: weed and crop --> Done in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffba2d",
   "metadata": {},
   "source": [
    "#### 4. Getting the appropriate paths for further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cea908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Crop\"\n",
    "class2_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Weed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e09ae",
   "metadata": {},
   "source": [
    "#### 5. Resize all the images(weeds & crops) to 400*400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Crop\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        resized_image = image.resize((400, 400))\n",
    "        resized_image.save(image_path)\n",
    "        image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Weed\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        resized_image = image.resize((400, 400))\n",
    "        resized_image.save(image_path)\n",
    "        image.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111ffd3",
   "metadata": {},
   "source": [
    "#### 6. Visualising the edges using Canny edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11a535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for Canny edge detection\n",
    "low_threshold = 50\n",
    "high_threshold = 150\n",
    "\n",
    "# Function to extract edge features from an image and visualize the edges\n",
    "def extract_and_visualize_edges(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "# Load the images and extract edge features\n",
    "for class_folder, class_label in [(class1_folder, 0), (class2_folder, 1)]:\n",
    "    for image_file in os.listdir(class_folder):\n",
    "        image_path = os.path.join(class_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        edges = extract_and_visualize_edges(image)\n",
    "\n",
    "        # Plot the original image and the extracted edges\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(edges, cmap=\"gray\")\n",
    "        axes[1].set_title(\"Extracted Edges\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ab4e1",
   "metadata": {},
   "source": [
    "#### 7. Extracting the edges from the images and storing them with the labels in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c92b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images and extract edge features\n",
    "edge_features = []\n",
    "labels = []\n",
    "desired_size = (400, 400) \n",
    "\n",
    "for class_folder, class_label in [(class1_folder, 0), (class2_folder, 1)]:\n",
    "    for image_file in os.listdir(class_folder):\n",
    "        image_path = os.path.join(class_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        resized_image = cv2.resize(image, desired_size)\n",
    "\n",
    "        gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(gray, threshold1=30, threshold2=100)\n",
    "\n",
    "        # Check if the number of edge features matches the expected length\n",
    "        expected_length = desired_size[0] * desired_size[1]\n",
    "        if len(edges.flatten()) == expected_length:\n",
    "            # Append the edge features and label to the lists\n",
    "            edge_features.append(edges.flatten())\n",
    "            labels.append(class_label)\n",
    "        else:\n",
    "            # Skip the image with mismatched edge features length\n",
    "            print(f\"Skipping image {image_file} due to mismatched edge features length\")\n",
    "            print(f\"Expected length: {expected_length}, Actual length: {len(edges.flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d604a5",
   "metadata": {},
   "source": [
    "#### 8. Convert the features to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to numpy arrays\n",
    "X = np.array(edge_features)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa171f",
   "metadata": {},
   "source": [
    "#### 9. Split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e850877",
   "metadata": {},
   "source": [
    "#### 10. Train SVM on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab04c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Support Vector Machine (SVM) model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f31fe",
   "metadata": {},
   "source": [
    "#### 11. Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8a16e",
   "metadata": {},
   "source": [
    "#### 12. Evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abfe5c",
   "metadata": {},
   "source": [
    "#### 13. Testing with a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test image\n",
    "test_image = cv2.imread(\"agri_0_136.jpeg_1.jpg\")\n",
    "desired_size = (400, 400)\n",
    "\n",
    "# Resize the test image to the desired size\n",
    "resized_test_image = cv2.resize(test_image, desired_size)\n",
    "\n",
    "# Convert the test image to grayscale\n",
    "gray_test = cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Canny edge detection to the test image\n",
    "test_edges = cv2.Canny(gray_test, threshold1=30, threshold2=100)\n",
    "\n",
    "# Reshape the test image features to match the input shape of the model\n",
    "test_features = test_edges.flatten().reshape(1, -1)\n",
    "\n",
    "# Make predictions on the test image\n",
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_class = y_pred[0]\n",
    "\n",
    "# Map the class label to a meaningful class name (if applicable)\n",
    "class_names = [\"weed\", \"crop\"]  # Define your class names\n",
    "predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "# Print the predicted class label and name\n",
    "print(\"Predicted Class Label:\", predicted_class)\n",
    "print(\"Predicted Class Name:\", predicted_class_name)\n",
    "\n",
    "# Display the test image and the prediction label using Matplotlib\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].imshow(cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Test Image\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].text(0.5, 0.5, f\"Predicted Class: {predicted_class_name}\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf2cf7",
   "metadata": {},
   "source": [
    "### Improving the above model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340701d9",
   "metadata": {},
   "source": [
    "#### 1. Extracting the edges from the images using adaptive thresholding, dilation and canny edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3ebc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for Canny edge detection\n",
    "low_threshold = 75\n",
    "high_threshold = 100\n",
    "\n",
    "# Function to extract edge features from an image and visualize the edges\n",
    "def extract_and_visualize_edges(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to obtain binary image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Perform morphological operations to enhance the foreground regions\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Use Canny edge detection on the foreground regions\n",
    "    edges = cv2.Canny(sure_bg, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "# Absolute paths for the \"Crop\" and \"Weed\" folders\n",
    "crop_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Crop\"\n",
    "weed_folder = r\"C:\\Users\\medad\\OneDrive - Mahindra University\\MU_ThirdYear\\Sem2\\DIP\\Project\\archive\\agri_data\\Cropped\\Weed\"\n",
    "# Function to display the images and extracted edges\n",
    "def display_image_with_edges(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    edges = extract_and_visualize_edges(image)\n",
    "\n",
    "    # Plot the original image and the extracted edges\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(edges, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Extracted Edges\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print the results for the first five images in the \"Crop\" folder\n",
    "crop_images = os.listdir(crop_folder)[:5]\n",
    "for image_name in crop_images:\n",
    "    image_path = os.path.join(crop_folder, image_name)\n",
    "    display_image_with_edges(image_path)\n",
    "\n",
    "# Print the results for the first five images in the \"Weed\" folder\n",
    "weed_images = os.listdir(weed_folder)[:5]\n",
    "for image_name in weed_images:\n",
    "    image_path = os.path.join(weed_folder, image_name)\n",
    "    display_image_with_edges(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aabd73",
   "metadata": {},
   "source": [
    "#### 2. Extract the edge features and store them in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and extract edge features\n",
    "edge_features = []\n",
    "labels = []\n",
    "\n",
    "for class_folder, class_label in [(crop_folder, 0), (weed_folder, 1)]:\n",
    "    for image_file in os.listdir(class_folder):\n",
    "        image_path = os.path.join(class_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        edges = extract_and_visualize_edges(image)\n",
    "\n",
    "        # Check if the number of edge features matches the expected length\n",
    "        expected_length = edges.shape[0] * edges.shape[1]\n",
    "        if len(edges.flatten()) == expected_length:\n",
    "            # Append the edge features and label to the lists\n",
    "            edge_features.append(edges.flatten())\n",
    "            labels.append(class_label)\n",
    "        else:\n",
    "            # Skip the image with mismatched edge features length\n",
    "            print(f\"Skipping image {image_file} due to mismatched edge features length\")\n",
    "            print(f\"Expected length: {expected_length}, Actual length: {len(edges.flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfc2b9",
   "metadata": {},
   "source": [
    "#### 3. Convert them to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to numpy arrays\n",
    "edge_features = np.array(edge_features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ed0fb",
   "metadata": {},
   "source": [
    "#### 4. Display their dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the edge features and labels arrays\n",
    "print(\"Edge Features Shape:\", edge_features.shape)\n",
    "print(\"Labels Shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf0efd",
   "metadata": {},
   "source": [
    "#### 5. Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6befd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(edge_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e583a",
   "metadata": {},
   "source": [
    "#### 6. Train the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6646aa8",
   "metadata": {},
   "source": [
    "#### 7. Predict on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a8550",
   "metadata": {},
   "source": [
    "#### 8. Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618342d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "classes = ['Crop', 'Weed']\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02406fc4",
   "metadata": {},
   "source": [
    "#### 9. Test with some test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test image\n",
    "test_image = cv2.imread(\"agri_0_136.jpeg_1.jpg\")\n",
    "desired_size = (400, 400)\n",
    "\n",
    "resized_test_image = cv2.resize(test_image, desired_size)\n",
    "\n",
    "gray_test = cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Canny edge detection to the test image\n",
    "test_edges = cv2.Canny(gray_test, threshold1=30, threshold2=100)\n",
    "\n",
    "test_features = test_edges.flatten().reshape(1, -1)\n",
    "\n",
    "y_pred = svm.predict(test_features)\n",
    "\n",
    "predicted_class = y_pred[0]\n",
    "\n",
    "# Map the class label to a meaningful class name (if applicable)\n",
    "class_names = [\"Weed\", \"Crop\"]  # Define your class names\n",
    "predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "print(\"Predicted Class Label:\", predicted_class)\n",
    "print(\"Predicted Class Name:\", predicted_class_name)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].imshow(cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Test Image\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].text(0.5, 0.5, f\"Predicted Class: {predicted_class_name}\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test image\n",
    "test_image = cv2.imread(\"Weed.png\")\n",
    "desired_size = (400, 400)\n",
    "\n",
    "resized_test_image = cv2.resize(test_image, desired_size)\n",
    "\n",
    "gray_test = cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "test_edges = cv2.Canny(gray_test, threshold1=30, threshold2=100)\n",
    "\n",
    "# Reshape the test image features to match the input shape of the model\n",
    "test_features = test_edges.flatten().reshape(1, -1)\n",
    "\n",
    "y_pred = svm.predict(test_features)\n",
    "\n",
    "predicted_class = y_pred[0]\n",
    "\n",
    "# Map the class label to a meaningful class name (if applicable)\n",
    "class_names = [\"Weed\", \"Crop\"]  # Define your class names\n",
    "predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "print(\"Predicted Class Label:\", predicted_class)\n",
    "print(\"Predicted Class Name:\", predicted_class_name)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].imshow(cv2.cvtColor(resized_test_image, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title(\"Test Image\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].text(0.5, 0.5, f\"Predicted Class: {predicted_class_name}\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
